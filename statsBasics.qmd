# Statistical Notation

## Sums

\begin{equation}
\begin{split}
\sum_{i=1}^{3}{X_i} &= X_1 + X_2 + X_3\\
\end{split}
\end{equation}



Example:

The equation

\begin{equation}
\begin{split}
\hat{p} &= \frac{\sum\limits_N{Cr(r-1)}}{\sum\limits_N{Cr(s-1)}}\\
\end{split}
\end{equation}

would be more clearly written as

\begin{equation}
\begin{split}
\hat{p} &= \frac{\sum\limits_{i=1}^N{C_ir_i(r_i-1)}}{\sum\limits_{i=1}^N{C_ir_i(s_i-1)}}\\
\end{split}
\end{equation}

## Products

\begin{equation}
\begin{split}
\prod_{i=1}^{3}{X_i} &= X_1 \times X_2 \times X_3\\
\end{split}
\end{equation}

## Likelihood

*Based in part on my class notes from the 1988 UCLA Biomathematics 207 A course "Theoretical Genetic Modeling" taught by Dr. Susan Hodge.* 

The likelihood $L(Hypothesis\ H | Data\ D)$, given a specific model, is proportional to $P(D|H)$ with the constant of proportionality being arbitrary.

As such, the likelihood is a function of the hypothesis, not of the data. 

As Etz (2018) states: 

"The likelihood of a hypothesis (H) given some data (D) is the probability of obtaining D given that H is true multiplied by an arbitrary positive constant K: $L(H) = K × P(D|H)$. ... For likelihood, the data are treated as a given, and the hypothesis varies." (Etz, 2018, p. 60)

Etz A. Introduction to the Concept of Likelihood and Its Applications. Advances in Methods and Practices in Psychological Science. SAGE Publications Inc; 2018 Mar 1;1(1):60–69. DOI: <https://doi.org/10.1177/2515245917744314>


## Conditional Probability

1. Definition:

\begin{equation}
\begin{split}
P(A|B) &= \frac{P(A \cap B)}{P(B)}\\
\end{split}
\end{equation}

2. Let $S = \bigcup\limits_{i=1}^nA_i$ and $A_i \cap A_j = \emptyset$ for $i \ne j$.  Then $P(B) = \sum\limits_{i=1}^nP(B|A_i)P(A_i)$

3. $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

4. Bayes Rule

Let $S = \bigcup\limits_{i=1}^nA_i$ and $A_i \cap A_j = \emptyset$ for $i \ne j$.  Then $P(A_j|B) =\frac{P(B|A_j)P(A_j)}{\sum\limits_{i=1}^nP(B|A_i)P(A_i)}$

5. Intermediate Conditioning

\begin{equation}
\begin{split}
P(A|B) = \sum\limits_{i}P(A|X_i,B)P(X_i|B)\\
\end{split}
\end{equation}

6. $P(A|B, C) = \frac{P(B|A,C)P(A|C)}{P(B|C)}$

7. $P(A,B|C) = P(A|B,C)P(B|C)$

8. If $A_i$ are mutually exclusive and exhaustive, then

\begin{equation}
\begin{split}
P(A_j|B, C) = \frac{P(B|A_j,C)P(A_j|C)}{\sum\limits_iP(B|A_i,C)P(A_i|C)}\\
\end{split}
\end{equation}